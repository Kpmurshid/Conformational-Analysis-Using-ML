{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdtraj as md\n",
    "import MDAnalysis as mda\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This section outlines the process of extracting relevant features from the molecular dynamics simulation data of the protein. Features like dihedral angles, number of hydrogen bonds, RMSD, Rg, and SASA are crucial descriptors of the protein's conformational states. By extracting these features, we can gain insights into the structural and dynamic properties of the protein over time._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diherdral Angles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Dihedral angles describe the rotations between atoms in a molecule, particularly focusing on backbone angles like phi (ϕ) and psi (ψ)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trajectory\n",
    "traj = md.load('md_0_1_noPBC.xtc', top='topology.pdb')\n",
    "\n",
    "# Compute phi and psi dihedral angles\n",
    "_, phi_angles = md.compute_phi(traj)\n",
    "_, psi_angles = md.compute_psi(traj)\n",
    "\n",
    "# Prepare the DataFrame for the dihedral angles\n",
    "phi_df = pd.DataFrame(phi_angles)\n",
    "psi_df = pd.DataFrame(psi_angles)\n",
    "angles_df = pd.concat([phi_df, psi_df], axis=1)\n",
    "\n",
    "#Remove the column labels\n",
    "angles_df.columns = range(angles_df.shape[1])\n",
    "\n",
    "# Save the angles DataFrame to a CSV file\n",
    "angles_df.to_csv('dihedral_angles.csv', index=False)\n",
    "\n",
    "# Plot the angles\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(phi_angles.shape[1]):\n",
    "    plt.plot(phi_angles[:, i], label=f'Phi {i + 1}')\n",
    "for i in range(psi_angles.shape[1]):\n",
    "    plt.plot(psi_angles[:, i], label=f'Psi {i + 1}')\n",
    "plt.xlabel('Frame')\n",
    "plt.ylabel('Dihedral Angle (radians)')\n",
    "plt.legend()\n",
    "plt.title('Phi and Psi Dihedral Angles')\n",
    "plt.savefig('dihedral_angles_plot.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angles_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hydrogen Bond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Tracking hydrogen bonds over time can reveal insights into the protein's folding and stability._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify hydrogen bonds using the wernet_nilsson method\n",
    "hbonds = md.wernet_nilsson(traj)\n",
    "\n",
    "# Count the number of hydrogen bonds in each frame\n",
    "hbond_counts = [len(frame_hbonds) for frame_hbonds in hbonds]\n",
    "\n",
    "# Create DataFrame for hydrogen bonds\n",
    "hbond_df = pd.DataFrame(hbond_counts)\n",
    "\n",
    "# Save hydrogen bond counts to a CSV file\n",
    "hbond_df.to_csv('hbond_counts.csv', index=False)\n",
    "\n",
    "# Plot the number of hydrogen bonds in each frame\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(hbond_df.index, hbond_df[0], label='Hydrogen Bonds', color='blue')\n",
    "plt.xlabel('Frame')\n",
    "plt.ylabel('Number of Hydrogen Bonds')\n",
    "plt.title('Number of Hydrogen Bonds in Each Frame')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('hbond_plot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbond_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radius of Gyration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Radius of gyration (Rg) measures the compactness of the protein structure by calculating the root mean square distance of atoms from their center of mass. In this section, Rg will be computed to analyze how the compactness of the protein changes during the simulation. Fluctuations in Rg can indicate folding and unfolding events or transitions between different conformational states._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the radius of gyration for each frame\n",
    "radii_of_gyration = md.compute_rg(traj)\n",
    "\n",
    "# Create DataFrame for radius of gyration\n",
    "rg_df = pd.DataFrame(radii_of_gyration)\n",
    "\n",
    "# Save the radius of gyration values to a CSV file\n",
    "rg_df.to_csv('Radius_Of_Gyration.csv', index=False)\n",
    "\n",
    "# Plot the radius of gyration over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(rg_df.index, rg_df[0], label='Radius of Gyration', color='blue')\n",
    "plt.xlabel('Frame')\n",
    "plt.ylabel('Radius of Gyration (nm)')\n",
    "plt.title('Radius of Gyration Over Time')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('radius_of_gyration_plot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_RMSD measures the average deviation of atomic positions in a protein over time relative to a reference structure. This section calculates the RMSD of the protein's backbone atoms to evaluate its structural stability during the simulation. A high RMSD might indicate significant conformational changes, while a low RMSD suggests a stable structure._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSD for all frames relative to the first frame\n",
    "rmsds = md.rmsd(traj, traj, 0)\n",
    "\n",
    "# Create DataFrame for RMSD\n",
    "rmsd_df = pd.DataFrame(rmsds)\n",
    "\n",
    "# Save RMSD values to a CSV file\n",
    "rmsd_df.to_csv('Rmsd_values.csv', index=False)\n",
    "\n",
    "# Plot RMSD|\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(rmsd_df.index, rmsd_df[0], label='RMSD')\n",
    "plt.xlabel('Frame')\n",
    "plt.ylabel('RMSD (nm)')\n",
    "plt.legend()\n",
    "plt.title('RMSD of Residues 40 to 56')\n",
    "plt.savefig('rmsd_plot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsd_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SASA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_SASA is a measure of the surface area of a protein that is accessible to the solvent (e.g., water molecules). This metric provides insights into the protein’s folding state, as buried residues typically indicate a folded structure, whereas exposed residues suggest unfolding. In this section, the SASA of the protein will be computed throughout the simulation to understand how solvent exposure changes with different conformational states._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the solvent accessible surface area (SASA) for each frame\n",
    "sasa = md.shrake_rupley(traj, probe_radius=0.14)  # Default probe_radius for water is 0.14 nm\n",
    "\n",
    "# Sum SASA of all atoms to get the total SASA per frame\n",
    "total_sasa_per_frame = np.sum(sasa, axis=1)\n",
    "\n",
    "# Create DataFrame for SASA\n",
    "sasa_df = pd.DataFrame(total_sasa_per_frame)\n",
    "\n",
    "# Save the total SASA values to a CSV file\n",
    "sasa_df.to_csv('sasa_values.csv', index=False)\n",
    "\n",
    "# Plot the total SASA over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sasa_df.index, sasa_df[0], label='Total SASA', color='blue')\n",
    "plt.xlabel('Frame')\n",
    "plt.ylabel('Total SASA (nm^2)')\n",
    "plt.title('Solvent Accessible Surface Area (SASA) Over Time')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('sasa_plot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sasa_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise Distance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The pairwise distance between backbone atoms is a critical structural feature that can provide detailed information on the spatial arrangement of the protein's structure. In this section, we calculate the pairwise distances between selected backbone atoms (such as Cα, C, N and 0 atoms) to capture subtle changes in the protein’s conformation. These distances are particularly useful for detecting shifts in secondary structure elements (e.g., turns, loops)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your trajectory and topology files\n",
    "u = mda.Universe('topology.pdb', 'md_0_1_noPBC.xtc')\n",
    "\n",
    "# Select the backbone atoms\n",
    "backbone_atoms = u.select_atoms('name N C O CA')\n",
    "\n",
    "# Initialize lists to store pairwise distances\n",
    "pairwise_distances = []\n",
    "\n",
    "# Loop over each frame in the trajectory\n",
    "for ts in u.trajectory:\n",
    "    # Get positions of backbone atoms\n",
    "    positions = backbone_atoms.positions\n",
    "    \n",
    "    # Ensure all frames have the same number of backbone atoms\n",
    "    n_atoms = len(positions)\n",
    "    if n_atoms == 0:\n",
    "        print(f\"Warning: No backbone atoms found in frame {ts.frame}. Skipping this frame.\")\n",
    "        continue\n",
    "\n",
    "    # Calculate pairwise distances\n",
    "    distances = np.linalg.norm(positions[:, np.newaxis, :] - positions[np.newaxis, :, :], axis=-1)\n",
    "    \n",
    "    # Add distances to the list\n",
    "    pairwise_distances.append(distances)\n",
    "\n",
    "# Convert the list to a NumPy array for distances\n",
    "pairwise_distances_arr = np.array(pairwise_distances)\n",
    "\n",
    "# Get the number of frames and atoms\n",
    "n_frames, n_atoms, _ = pairwise_distances_arr.shape\n",
    "\n",
    "# Initialize lists for flattened distances\n",
    "flattened_distances_list = []\n",
    "\n",
    "# Flatten the pairwise distances for each frame\n",
    "for distances in pairwise_distances_arr:\n",
    "    flattened_distances = [distances[i, j] for i in range(n_atoms) for j in range(i + 1, n_atoms)]\n",
    "    flattened_distances_list.append(flattened_distances)\n",
    "\n",
    "# Create a DataFrame with flattened distances, using numeric indices for columns\n",
    "df_distances = pd.DataFrame(flattened_distances_list)\n",
    "\n",
    "# Set index to represent frame numbers as 0, 1, 2, ...\n",
    "df_distances.index = list(range(n_frames))\n",
    "\n",
    "# Set column names to be numeric indices 0, 1, 2, ...\n",
    "df_distances.columns = list(range(df_distances.shape[1]))\n",
    "\n",
    "# Save the combined DataFrame to a CSV file\n",
    "#df_distances.to_csv('distance.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_distances.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_aggregating all the extracted features—including dihedral angles, hydrogen bonds, RMSD, Rg, SASA, and pairwise distances—into a single data file. This combined dataset will serve as the input for machine learning models,enabling us to classify and analyze the conformational complexity of the GB1 protein. The data will be preprocessed, normalized, and applied PCA to facilitate efficient analysis using clustering and classification._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combine all features into a single DataFrame\n",
    "combined_df = pd.concat([angles_df, hbond_df, rg_df, rmsd_df, sasa_df, df_distances], axis=1)\n",
    "\n",
    "# Save the combined DataFrame to a CSV file\n",
    "combined_df.to_csv(\"combined_features.csv\", index=True)\n",
    "print(\"Combined CSV file saved as 'combined_features.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardisation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Standardization is essential for machine learning models to ensure that all features are on a similar scale. Here Z-score standardization applied to the combined features dataset._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the combined features\n",
    "features = combined_df.values\n",
    "# Apply Z-score standardization\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "normalized_df= pd.DataFrame(scaled_features)\n",
    "\n",
    "normalized_df.to_csv(\"standardized_combined_features.csv\", index=False)\n",
    "print(\"Standardized CSV file saved as 'standardized_combined_features.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Principal Component Analysis (PCA) is applied to the standardized data to reduce the dimensionality while retaining the maximum amount of variance from the original features. The cumulative variance explained by each principal component is computed and plotted. This plot helps determine the optimal number of components to retain, based on the amount of variance we wish to capture, aiding in the identification of key patterns in the conformational data._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to the scaled data\n",
    "pca = PCA().fit(scaled_features)\n",
    "\n",
    "# Compute the cumulative variance explained by each component\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_) * 100\n",
    "\n",
    "# Plot the explained variance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(cumulative_variance)\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance (%)')\n",
    "plt.title('Explained Variance vs. Number of Components')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of components that explain at least 95% of the variance\n",
    "\n",
    "percentage_variance = 95\n",
    "n_components = np.argmax(cumulative_variance >= percentage_variance) + 1\n",
    "\n",
    "# Print the number of components\n",
    "print(f'Number of components chosen to explain {percentage_variance}% variance: {n_components}')\n",
    "\n",
    "# Apply PCA with the chosen number of components\n",
    "pca = PCA(n_components=n_components)\n",
    "principal_components = pca.fit_transform(scaled_features)\n",
    "reconstructed_data = pca.inverse_transform(principal_components)\n",
    "\n",
    "# Calculate reconstruction error (e.g., mean squared error)\n",
    "reconstruction_error = np.mean((scaled_features- reconstructed_data) ** 2)\n",
    "print(f'Reconstruction error: {reconstruction_error}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA and save the results\n",
    "pca = PCA(n_components=n_components)\n",
    "principal_components = pca.fit_transform(scaled_features)\n",
    "\n",
    "# Create DataFrame for PCA results\n",
    "pca_columns = [f'PC{i+1}' for i in range(principal_components.shape[1])]\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=pca_columns)\n",
    "\n",
    "pca_df.to_csv('pca_results.csv', index=False)\n",
    "\n",
    "# Plot all PCA components in a single figure\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Create a pair plot of the principal components\n",
    "sns.pairplot(pca_df, diag_kind='kde')\n",
    "\n",
    "plt.suptitle('Pair Plot of PCA Components', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_In this section, K-means clustering is applied to the PCA-transformed data to identify distinct conformational clusters of the protein. The process involves the following steps:_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Average Inertia Calculation: A function is defined to compute the average inertia (a measure of how tightly the data points are clustered) over multiple runs of K-means for different values of k (number of clusters).\n",
    "\n",
    "-  Elbow Method: The \"elbow\" point is identified by finding where the decrease in inertia begins to slow, suggesting the optimal number of clusters. The optimal value of k is determined using the elbow method, which visually inspects the plot of average inertia versus k.\n",
    "\n",
    "-   Clustering: K-means clustering is then performed using the optimal number of clusters, and the cluster labels are assigned to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.FInding optimal k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to calculate average inertia\n",
    "def calculate_average_inertia(z, k_range, n_runs=10):\n",
    "    avg_inertia = []\n",
    "    for k in k_range:\n",
    "        inertia_sum = 0\n",
    "        for _ in range(n_runs):\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "            kmeans.fit(z)\n",
    "            inertia_sum += kmeans.inertia_\n",
    "        avg_inertia.append(inertia_sum / n_runs)\n",
    "    return avg_inertia\n",
    "\n",
    "# Load PCA results from pca_df\n",
    "z = pca_df  # Use PCA-transformed data from pca_df\n",
    "\n",
    "# Define the range of k values\n",
    "k_range = range(1, 11)\n",
    "\n",
    "# Calculate average inertia\n",
    "inertia = calculate_average_inertia(z, k_range)\n",
    "\n",
    "# Function to find the optimal k (you can use a simpler approach without derivatives)\n",
    "def find_optimal_k(inertia):\n",
    "    # Find the \"elbow\" point where the decrease in inertia slows down\n",
    "    inertia_diff = np.diff(inertia)\n",
    "    inertia_diff2 = np.diff(inertia_diff)\n",
    "    elbow_index = np.argmin(inertia_diff2) + 1\n",
    "    return elbow_index + 1\n",
    "\n",
    "# Determine the optimal k using the smoothed elbow method\n",
    "optimal_k = find_optimal_k(inertia)\n",
    "print(f'Optimal number of clusters (k) based on the elbow method: {optimal_k}')\n",
    "\n",
    "# Plot the elbow curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, inertia, color='g', marker='o', linestyle='--', linewidth=3)\n",
    "plt.xlabel(\"Value of K\")\n",
    "plt.ylabel(\"Average Inertia\")\n",
    "plt.title('Elbow Method for Optimal k in K-means Clustering')\n",
    "plt.xticks(k_range)\n",
    "plt.grid(True)\n",
    "\n",
    "# Annotate the optimal k value on the plot\n",
    "plt.axvline(x=optimal_k, color='r', linestyle='--')\n",
    "plt.text(optimal_k, inertia[k_range.index(optimal_k)], f'Optimal k = {optimal_k}', color='red', fontsize=12, ha='right')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.k-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-means clustering using all PCA components and the optimal k\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "kmeans.fit(pca_df)  # Use all PCA components\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Add cluster labels to DataFrame\n",
    "pca_df['K-means Cluster'] = labels\n",
    "\n",
    "# Save the results with cluster labels\n",
    "pca_df.to_csv('kmeans_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the clusters using the first two principal components for visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='K-means Cluster', palette='viridis', data=pca_df, legend='full')\n",
    "plt.title(f'PCA of Protein Conformational States with K-means Clusters (k={optimal_k})')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_In this section, we prepare the data for classification using all the PCA components and classify the conformational states of the protein. The process involves:_\n",
    "\n",
    "- Data Splitting: We split the dataset into training and testing sets. The feature set X includes all PCA components, while the target variable y contains the cluster labels assigned by K-means. The data is split using an 80-20 ratio, where 80% is used for training and 20% for testing.\n",
    "\n",
    "- Defining Classifiers: We apply several classification algorithms—Support Vector Machines (SVM), K-Nearest Neighbors (KNN), Random Forest, and Logistic Regression. Each classifier is trained on the training set, and predictions are made on the test set.\n",
    "\n",
    "- Evaluation: The performance of each classifier is evaluated using accuracy, classification reports, and confusion matrices. The accuracy measures how well the classifier predicts the correct conformational cluster. Additionally, confusion matrices are plotted to visualize the classification performance, showing the actual vs. predicted labels.\n",
    "\n",
    "_This step helps to determine which machine learning algorithm is best suited for classifying the different conformational states of the protein based on the PCA-reduced features._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for classification using all PCA components\n",
    "X = pca_df.drop('K-means Cluster', axis=1)  # Exclude the cluster labels\n",
    "y = pca_df['K-means Cluster']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define classifiers\n",
    "classifiers = {\n",
    "    'SVM': SVC(),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Logistic Regression': LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "}\n",
    "\n",
    "accuracy_results = {}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracy_results[name] = accuracy\n",
    "    print(f'{name} Accuracy: {accuracy}')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'{name} Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the best algorithm and rank them\n",
    "best_algorithm = max(accuracy_results, key=accuracy_results.get)\n",
    "sorted_algorithms = sorted(accuracy_results.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "print(\"\\nBest Algorithm:\", best_algorithm)\n",
    "print(\"Algorithm Rankings:\")\n",
    "for rank, (name, accuracy) in enumerate(sorted_algorithms, start=1):\n",
    "    print(f'{rank}. {name}: {accuracy}')\n",
    "\n",
    "# Visualization of the algorithm rankings\n",
    "algorithms, accuracies = zip(*sorted_algorithms)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=list(algorithms), y=list(accuracies), hue=list(algorithms), palette='viridis', legend=False)\n",
    "plt.title('Algorithm Accuracy Rankings')\n",
    "plt.xlabel('Algorithm')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# Adding values on top of the bars\n",
    "for index, value in enumerate(accuracies):\n",
    "    plt.text(index, value, f'{value:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross-validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_In this section, we use cross-validation to further evaluate the performance and robustness of different classification algorithms. Cross-validation helps assess how well the model generalizes to unseen data by splitting the dataset into multiple subsets (folds) and testing the model on each fold._\n",
    "\n",
    "* Cross-Validation: The cross_val_score function is used to perform 4-fold cross-validation on each classifier. This involves splitting the data into 4 parts—training the model on 3 parts and validating it on the remaining part, iterating this process 4 times so that each fold is used as a validation set once.\n",
    "\n",
    "* Mean Accuracy: For each classifier (SVM, KNN, Random Forest, Logistic Regression), the cross-validation scores are printed along with the mean cross-validation accuracy. This gives an overall sense of how consistently the classifier performs across different subsets of the data.\n",
    "\n",
    "_This step provides a more reliable evaluation of the classifier's performance by reducing bias and variance in the results, ensuring that the models perform well on unseen data._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score , learning_curve\n",
    "# Perform cross-validation and print mean accuracy\n",
    "for name, clf in classifiers.items():\n",
    "    scores = cross_val_score(clf, X, y, cv=4)  \n",
    "    print(f'{name} Cross-Validation Scores:', scores)\n",
    "    print(f'Mean {name} Cross-Validation Score:', scores.mean())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Function to plot learning curves and print scores\n",
    "def plot_learning_curve_and_scores(estimator, title, X, y, cv=5, train_sizes=np.linspace(.1, 1.0, 5), random_state=42):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "\n",
    "    # Use StratifiedKFold if you have classification tasks\n",
    "    stratified_cv = StratifiedKFold(n_splits=cv, shuffle=True, random_state=random_state)\n",
    "\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = \\\n",
    "        learning_curve(estimator, X, y, cv=stratified_cv, train_sizes=train_sizes,\n",
    "                       return_times=True, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    \n",
    "    # Print cross-validation scores\n",
    "    scores = cross_val_score(estimator, X, y, cv=stratified_cv)\n",
    "    print(f'{title} Cross-Validation Scores:', scores)\n",
    "    print(f'Mean {title} Cross-Validation Score:', scores.mean())\n",
    "    print()\n",
    "\n",
    "# Example of how to use the function with classifiers\n",
    "for name, clf in classifiers.items():\n",
    "    plot_learning_curve_and_scores(clf, f'Learning Curve for {name}', X, y, cv=5, random_state=42)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_In this section, we utilize learning curves to analyze the performance of different classifiers as the training data size increases. Learning curves provide insight into the model’s capacity, helping to understand whether a classifier is underfitting or overfitting. Additionally, we use Stratified K-Fold cross-validation to maintain the balance of class distributions across folds, which is particularly important for classification tasks with imbalanced data._\n",
    "\n",
    "- Learning Curves: The plot_learning_curve_and_scores function is defined to plot the learning curve for each classifier. The learning curve shows how the training score and cross-validation score change as the training set size increases. \n",
    "- Stratified K-Fold Cross-Validation: The StratifiedKFold technique ensures that each fold in the cross-validation process maintains the same class distribution as the original dataset, which improves the reliability of model evaluation, especially when working with classification problems.\n",
    "\n",
    "- Model Performance: For each classifier (SVM, KNN, Random Forest, and Logistic Regression), learning curves are plotted, and the cross-validation scores are printed. These curves help visualize how well the model generalizes with increasing amounts of training data and how its performance stabilizes across different data sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selection and Visualization of Representative Conformations from Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_In this section, we identify the best representative frame from each cluster and extract the corresponding conformation from the molecular dynamics trajectory. This allows us to visually examine the key conformational states of the protein._\n",
    "\n",
    "- Finding Best Representative Frames:\n",
    "        The centroid of each cluster, computed during K-means clustering, is considered the most representative point for that cluster.\n",
    "        For each cluster, we calculate the Euclidean distance between every frame in the cluster and the centroid using the cdist function from scipy.spatial.distance.\n",
    "        The frame with the smallest distance to the centroid is selected as the best representative of that cluster.\n",
    "\n",
    "- Extracting Best Conformations:\n",
    "        The best representative frames are extracted from the molecular dynamics trajectory (stored in traj), and these frames are saved to a new trajectory file (best_conformations.xtc) for further analysis.\n",
    "\n",
    "- Visualization:\n",
    "        Using NGLview, the best conformations are visualized as ribbon representations, providing an intuitive way to compare the different structural states.\n",
    "        Each conformation is rendered in a 3D viewer, allowing for interactive exploration of the protein structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Get the centroids of the clusters from the K-means model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Find the closest frame to each centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "best_frames = []\n",
    "\n",
    "for i, centroid in enumerate(centroids):\n",
    "    # Get all frames in this cluster\n",
    "    cluster_frames = pca_df[pca_df['K-means Cluster'] == i].drop('K-means Cluster', axis=1)\n",
    "    \n",
    "    # Compute the distance of each frame in the cluster to the centroid\n",
    "    distances = cdist(cluster_frames, [centroid], metric='euclidean')\n",
    "    \n",
    "    # Find the index of the closest frame to the centroid\n",
    "    best_frame_idx = cluster_frames.index[np.argmin(distances)]\n",
    "    \n",
    "    # Store the best frame index\n",
    "    best_frames.append(best_frame_idx)\n",
    "\n",
    "print(\"Best frames that represent each cluster:\", best_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.Extract the Best Conformations from the Trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the best conformations based on the frame indices\n",
    "best_conformations = traj[best_frames]\n",
    "\n",
    "# Save the best conformations to a new trajectory file \n",
    "best_conformations.save_xtc('best_conformations.xtc')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.Visualize the Best Conformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install nglview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nglview as nv\n",
    "# Create a list to store views\n",
    "views = []\n",
    "\n",
    "# Create a view for each best conformation and add it to the list\n",
    "for i, frame in enumerate(best_conformations):\n",
    "    view = nv.show_mdtraj(frame)\n",
    "    view.add_representation('ribbon')\n",
    "    # view.add_representation('surface')\n",
    "    views.append(view)\n",
    "\n",
    "# Display all views\n",
    "for view in views:\n",
    "    display(view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.Save each conformation as a separate PDB file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, frame in enumerate(best_conformations):\n",
    "    pdb_filename = f'best_conformation_{i}.pdb'\n",
    "    frame.save(pdb_filename)\n",
    "    print(f'Saved: {pdb_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
